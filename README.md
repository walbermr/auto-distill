# âš—ï¸ Auto-Distill

A collection of lightweight, minimalist distillation architectures. Designed for automatically test lightweight layers on different tasks. The priority is clarity, speed, and low resource usage â€” ideal for research prototypes, edge devices, or anyone who wants to learn by reading clean code.

## âœ¨ Features

âœ… Compact implementations â€” no unnecessary layers or complexity

- ğŸš€ Fast and memory-efficient
- ğŸ§° PyTorch backend
- ğŸ“š Well-documented and easy to extend
- ğŸ” Includes training scripts & evaluation utilities

## ğŸ“‚ Repository Structure

    auto_distill/
    â”œâ”€â”€ models/                 # Core model definitions
    â”‚   â”œâ”€â”€ convolution/        # Implementation of convolutional architectures
    â”‚       â”œâ”€â”€ separable/      # Separable convolution architectures
    â”œâ”€â”€ scripts/                # Training & evaluation scripts
    |   â”œâ”€â”€ classification      # Training task
    â””â”€â”€ requirements.txt        # Python dependencies